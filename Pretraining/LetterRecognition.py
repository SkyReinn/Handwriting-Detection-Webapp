# -*- coding: utf-8 -*-
"""LetterRecognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LwXw93GXyHzFxvYIHTRTMLOPA7gDrsZY
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras.models import Sequential
from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, Dropout

# Load EMNIST dataset
ds_train, ds_test = tfds.load('emnist/letters', split=['train', 'test'], shuffle_files=True, as_supervised=True, batch_size=64)

# Define function to normalize pixel values
def normalize_img(image, label):
    image = tf.cast(image, tf.float32) / 255.
    image = tf.round(image)
    image = tf.cast(image * 255, tf.uint8)
    return image, label
  
# Define a function to rotate and flip the images
def rotate_img(image, label):
    return tf.image.flip_left_right(tf.image.rot90(image, k=3)), label

# Apply normalization to train and test datasets
ds_train = ds_train.map(normalize_img)
ds_test = ds_test.map(normalize_img)
ds_train = ds_train.map(rotate_img)
ds_test = ds_test.map(rotate_img)

# Define function to one-hot encode labels
def one_hot_encode(image, label):
    return image, tf.one_hot(label-1, depth=26, dtype=tf.int32)

# Apply one-hot encoding to train and test datasets
ds_train = ds_train.map(one_hot_encode)
ds_test = ds_test.map(one_hot_encode)

# Shuffle the training dataset
ds_train_shuffled = ds_train.shuffle(buffer_size=10000)

# Select a random image and see it's shape and one-hot label
for image, label in ds_train_shuffled.take(1):
    print("Image shape:", image.shape)
    print("Label shape:", label.shape)
    plt.imshow(image[0,:,:,0], cmap='gray')
    print("One-hot encoded label:", label[0].numpy())

# Create model structure
model = Sequential()

model.add(Conv2D(32, (3,3), activation='relu', input_shape=(28,28,1)))
model.add(MaxPooling2D((2,2)))

model.add(Conv2D(64, (3,3), activation='relu', input_shape=(28,28,1)))
model.add(MaxPooling2D((2,2)))

model.add(Conv2D(128, (3,3), activation='relu', input_shape=(28,28,1)))
model.add(MaxPooling2D((2,2)))

model.add(Dropout(0.2))

model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(26, activation='softmax'))

# Build the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Model summary
model.summary()

# Train the model
history = model.fit(ds_train, epochs=10, batch_size=64, validation_data=ds_test)

# Evaluate the model
performance = model.evaluate(ds_test, batch_size=64)

# Shuffle the testing dataset
test_image = ds_test.shuffle(buffer_size=1000).take(1)

# Take a random image and print out the image and it's prediction
for image, label in test_image:
    prediction = np.argmax(model.predict(image)[0])
    letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']
    prediction = letters[prediction]
    plt.imshow(image[0])
    plt.show()
    print("The prediction of the image is", prediction)

from google.colab import files

# Save and download the model
model.save('LetterRecognitionModel.h5')
files.download('LetterRecognitionModel.h5')